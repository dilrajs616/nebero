---
layout: default
title: "Step 1: Web Scraping"
---

As the first task of our project we had to scrape all the 
In our web scraping project, we tried multiple libraries and tools to extract content from websites. Below is a detailed explanation of each approach we explored, including how they work, their pros, and cons.

## 1. Requests Library

### How It Works:
The `requests` library is a simple and powerful Python tool for sending HTTP requests. It allows you to make GET, POST, and other types of HTTP requests to a website and retrieve the HTML content of the page.

### Code Example:
```python
import requests

response = requests.get("https://example.com")
html_content = response.text
```

### Pros:
- Very simple to use and understand.
- Fast for scraping static content.
- Lightweight and doesn't use a lot of system resources.

### Cons:
- Cannot handle JavaScript rendering. Some websites use JavaScript to load content dynamically, and `requests` can’t scrape such sites because it only fetches the initial HTML.

## 2. Pyppeteer

### How It Works:
`pyppeteer` is a Python port of the `puppeteer` library (a Node.js tool) used for web scraping and automation. It works by launching a headless (non-UI) browser and simulating human-like interactions with the website, such as clicking buttons and waiting for JavaScript to load the content.

### Code Example:
```python
from pyppeteer import launch

async def fetch_content():
    browser = await launch(headless=True)
    page = await browser.newPage()
    await page.goto("https://example.com")
    content = await page.content()
    await browser.close()
    return content
```

### Pros:
- Can scrape dynamic websites that use JavaScript to load content.
- Simulates real user interactions, so it's effective for complex websites.

### Cons:
- Slower than `requests` since it requires launching a full browser.
- Not as reliable as other tools for scraping certain types of websites, particularly websites with heavy anti-scraping mechanisms.

## 3. Selenium

### How It Works:
`selenium` is another popular tool for automating web browsers. It allows you to control a browser (such as Chrome or Firefox) to interact with websites, just like a real user would. Selenium can handle dynamic content by running JavaScript and simulating clicks or keyboard inputs.

### Code Example:
```python
from selenium import webdriver

driver = webdriver.Chrome(executable_path='/path/to/chromedriver')
driver.get("https://example.com")
content = driver.page_source
driver.quit()
```

### Pros:
- Can handle JavaScript-heavy websites and dynamic content.
- Works well for websites that require interactions like clicks or form submissions.

### Cons:
- Slower than `requests` and `pyppeteer` due to the overhead of controlling a real browser.
- Consumes a lot of memory and resources.
- As the number of websites increases, scraping becomes slower and less efficient.

## 4. Playwright

### How It Works:
`Playwright` is a modern alternative to `Selenium` and `pyppeteer`. Like those tools, it allows you to automate browsers and scrape dynamic content, but it’s more efficient and faster. Playwright supports multiple browsers (Chromium, Firefox, and WebKit) and provides more control over interactions like clicking, scrolling, and waiting for elements to appear.

### Code Example:
```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch()
    page = browser.new_page()
    page.goto("https://example.com")
    content = page.content()
    browser.close()
```

### Pros:
- Faster and more efficient than Selenium and pyppeteer.
- Handles JavaScript-heavy websites with ease.
- Allows for scraping across multiple browsers (Chromium, Firefox, WebKit).
- Better performance with less memory usage.
- Designed for parallelism.

### Cons:
- Still has challenges with bypassing advanced anti-bot mechanisms like Cloudflare.
- It is very new compared to rest of the libraries. It was released in 2020. So it has a lot of issues. The biggest issue is of memory leakage. It takes very less memory. But over a long period of time, it keeps acquiring more and more memory and eventually it will cause the device crash due to no more memory error.

## Challenges: Cloudflare and Anti-bot Layers

While Playwright works well for most sites, we are still facing challenges with websites protected by Cloudflare or other anti-bot measures. These systems often detect scraping attempts and block or limit access, making it difficult to scrape data without encountering errors or CAPTCHA challenges.

### Solutions We Are Considering:
- Using IP rotation or proxies to avoid detection.
- Solving CAPTCHA with services like 2Captcha.
- Trying additional techniques like headless browser fingerprinting and setting custom user agents.

