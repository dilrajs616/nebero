---

layout: default  
title: "K-Means and DBSCAN Clustering: Detailed Procedure and Silhouette Score"

---

## Overview

In this section, we document our experience and the detailed steps for applying **K-Means** and **DBSCAN** clustering algorithms to our dataset. We also explain the **Silhouette Score**, which we used to evaluate the clustering performance. Despite using both algorithms, we observed that the clustering results were not as expected, with the best score achieved from **K-Means** being only **0.2**.

---

## K-Means Clustering

### 1. **What is K-Means Clustering?**

**K-Means** is one of the most commonly used clustering algorithms. The algorithm divides the dataset into **k** clusters based on the similarity of data points. The key idea behind K-Means is that each data point belongs to the cluster whose center (mean) is nearest. Here’s a basic summary of how it works:
- **Initialization**: K centroids are chosen randomly or using a heuristic.
- **Assignment**: Each data point is assigned to the nearest centroid.
- **Update**: New centroids are calculated as the mean of the points assigned to each cluster.
- **Repeat**: The assignment and update steps are repeated until convergence (i.e., when centroids no longer move significantly).

### 2. **Step-by-Step K-Means Procedure**

Here’s the procedure we followed to apply K-Means clustering:
1. **Preprocessing**: We performed the necessary preprocessing steps, including feature extraction (TF-IDF for text data), stop-word removal, and normalization.
2. **Choosing K**: The number of clusters **k** needs to be chosen beforehand. We initially experimented with multiple values of **k**, starting from 5 up to 20.
3. **Fitting the Model**: We used the K-Means algorithm with **k=10** to cluster our data.
4. **Evaluating with Silhouette Score**: We calculated the **Silhouette Score** to evaluate the quality of the clusters.
    - The **Silhouette Score** for K-Means clustering was **0.2**.
5. **Results**: The clustering results were not ideal, and the **Silhouette Score** indicated that the clusters were not well-defined. A score of 0.2 is considered to be very low, meaning that the data points were not well separated.

### 3. **Silhouette Score: Explanation**

The **Silhouette Score** is a measure of how well-separated the clusters are. It combines two concepts:
- **Cohesion**: How close the points within a cluster are to each other.
- **Separation**: How far away a cluster is from the next nearest cluster.

The **Silhouette Score** ranges from **-1 to 1**:
- **+1**: Indicates that the clusters are well-separated, and data points are correctly assigned to the appropriate cluster.
- **0**: Indicates that the data points are on the boundary between clusters, and the algorithm couldn’t clearly separate them.
- **-1**: Indicates poor clustering, where the data points might have been assigned to the wrong clusters.

In our case, with a score of **0.2**, the results suggested that the clustering was weak and not well-defined. We tried multiple values of **k**, but the score remained low.

---

## DBSCAN Clustering

### 1. **What is DBSCAN?**

**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that groups data points based on their density. Unlike K-Means, DBSCAN does not require the number of clusters to be specified in advance and is better suited for detecting clusters of arbitrary shape. It operates by defining regions of high point density as clusters and regions of low point density as noise (outliers). The key parameters of DBSCAN are:
- **eps**: The maximum distance between two points to be considered neighbors.
- **min_samples**: The minimum number of points required to form a dense region (a cluster).

### 2. **Step-by-Step DBSCAN Procedure**

We applied DBSCAN clustering after trying K-Means. Here’s the procedure we followed:
1. **Preprocessing**: The same preprocessing steps were applied to the data as in K-Means (feature extraction, stop-word removal, etc.).
2. **Choosing Parameters**: We experimented with various values of **eps** and **min_samples**. We began with **eps=0.5** and **min_samples=5** based on common recommendations.
3. **Fitting the Model**: We applied DBSCAN on our dataset using the selected parameters.
4. **Evaluating with Silhouette Score**: After clustering, we calculated the **Silhouette Score** to evaluate how well DBSCAN performed.
    - The **Silhouette Score** for DBSCAN was worse than K-Means and was even lower than **0.2**.
5. **Results**: The clustering results were even poorer with DBSCAN, and the low Silhouette Score indicated that the algorithm had not performed well at all. The model either detected too few clusters or classified many points as noise, contributing to the low score.

---

## Comparing K-Means and DBSCAN

### 1. **Strengths of K-Means**
- Works well when the clusters are spherical and well-separated.
- Fast and efficient for small to medium-sized datasets.
- Simple to implement and understand.

### 2. **Strengths of DBSCAN**
- No need to specify the number of clusters beforehand.
- Handles clusters of arbitrary shape well.
- Can identify outliers as noise, which is useful when the dataset contains noise.
  
### 3. **Why Did DBSCAN Perform Worse?**
- **Choice of Parameters**: DBSCAN requires careful tuning of the **eps** and **min_samples** parameters. A poor choice of these parameters can lead to bad results, such as detecting too few clusters or misclassifying points as noise.
- **Density Variability**: If the density of clusters is uneven across the data, DBSCAN can struggle. It works best when all clusters have a similar density, which may not always be the case with real-world data.
  
### 4. **Key Differences**
- **K-Means** requires the number of clusters to be specified beforehand and assumes spherical, equally sized clusters. On the other hand, **DBSCAN** doesn’t require the number of clusters to be specified and can handle arbitrary cluster shapes, making it more flexible in some cases.
- **DBSCAN** is sensitive to the **eps** parameter, and choosing the wrong value can lead to poor results.

---

## Conclusion

While both **K-Means** and **DBSCAN** are powerful clustering techniques, they didn’t perform as expected on our dataset. The **Silhouette Score** was low for both algorithms, indicating poor cluster separation. Here’s a summary:
- **K-Means** gave us a **Silhouette Score of 0.2**, suggesting weak clustering.
- **DBSCAN** performed worse, producing a lower Silhouette Score.
  
Despite these challenges, understanding how each algorithm works and its strengths and weaknesses is essential for troubleshooting and improving clustering outcomes. In the next steps, we will explore more advanced techniques and fine-tune the parameters of DBSCAN to see if we can improve the results.

