<h1 id="multilingual-data-translation-and-model-evaluation">Multilingual Data Translation and Model Evaluation</h1>

<h2 id="introduction">Introduction</h2>

<p>Today, we focused on addressing the issues we were encountering with processing <strong>multi-lingual data</strong>. After evaluating various options, we found and tested the <strong>M2M100 translation model</strong> from Hugging Face. This model offers robust translation capabilities across multiple languages, making it a strong candidate for our project.</p>

<h3 id="what-is-m2m100">What is M2M100?</h3>

<p>The <strong>M2M100 model</strong> is a <strong>many-to-many multilingual translation model</strong> developed by Facebook AI. Unlike traditional translation models, which typically translate between English and other languages (one-to-one or one-to-many), M2M100 supports direct translations between any pair of supported languages without needing English as an intermediary.</p>

<ul>
  <li><strong>Direct Language Pair Translation</strong>: The model can translate directly between any two languages (e.g., French to German, Hindi to Spanish), which allows for more accurate and context-preserving translations.</li>
  <li><strong>Model Size</strong>: M2M100 comes in various sizes, with the base version supporting more than 100 languages, making it highly versatile for multilingual projects.</li>
</ul>

<p>The model has been <strong>fine-tuned by multiple contributors</strong> to enhance its performance, and we shortlisted two specific fine-tuned models today, each having its own pros and cons.</p>

<h2 id="shortlisted-models">Shortlisted Models</h2>

<h3 id="1-helsinki-nlpopus-mt-src_lang-en">1. <strong>Helsinki-NLP/opus-mt-{src_lang}-en</strong></h3>

<ul>
  <li><strong>Description</strong>: This model requires the replacement of <code class="language-plaintext highlighter-rouge">{src_lang}</code> with the appropriate language code of the input data (e.g., <code class="language-plaintext highlighter-rouge">fr-en</code> for French to English). It downloads a specific model for each language, ensuring that only the required translation model is loaded into memory.</li>
  <li><strong>Size</strong>: Each language-specific model is <strong>300-400 MB</strong> in size.</li>
  <li><strong>VRAM Requirements</strong>: For a single website, only one model will be loaded into the GPU at a time, meaning that only <strong>300-400 MB of VRAM</strong> is required. The computational power is minimal, making it an efficient option for handling translations where only one language is processed at a time.</li>
  <li><strong>Pros</strong>:
    <ul>
      <li>Lower VRAM consumption.</li>
      <li>Scalable when dealing with fewer languages.</li>
      <li>Optimized for specific language pairs.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>A separate model needs to be downloaded for each language pair, increasing management overhead when handling many different languages.</li>
    </ul>
  </li>
</ul>

<h3 id="2-m2m100-fine-tuned-by-facebook">2. <strong>M2M100 Fine-Tuned by Facebook</strong></h3>

<ul>
  <li><strong>Description</strong>: This version of the <strong>M2M100 model</strong> was fine-tuned by Facebook and supports translations between <strong>any language pair</strong>. Unlike the Helsinki model, which requires separate models for each language, this model is a <strong>single translation model</strong> that handles all languages at once.</li>
  <li><strong>Size</strong>: The model is <strong>4.6 GB</strong> in size.</li>
  <li><strong>VRAM Requirements</strong>: Since it’s a larger model, it requires <strong>4.6 GB of VRAM</strong> to load, which can strain resources if multiple translations are needed at the same time.</li>
  <li><strong>Pros</strong>:
    <ul>
      <li>One model covers all languages, simplifying management.</li>
      <li>Capable of handling <strong>any-to-any</strong> translation.</li>
    </ul>
  </li>
  <li><strong>Cons</strong>:
    <ul>
      <li>High VRAM usage, which may require more powerful GPUs to run efficiently.</li>
      <li>Higher computational power needed due to its size.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>We have shortlisted two models to handle our multi-lingual data translation tasks. Both models offer unique advantages depending on the project’s requirements.</p>

<ul>
  <li><strong>Helsinki-NLP/opus-mt-{src_lang}-en</strong> is ideal for scenarios where only one or a few languages are involved, as it offers lower resource usage and is highly efficient in terms of VRAM consumption.</li>
  <li><strong>M2M100 fine-tuned by Facebook</strong> is a versatile, all-in-one solution that can handle any-to-any language translations but comes with the trade-off of higher VRAM and computational needs.</li>
</ul>

<p>Next, we will evaluate these models further to decide which best suits our project’s long-term needs.</p>

