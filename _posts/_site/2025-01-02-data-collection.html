<h1 id="step-0-data-collection">Step 0: Data Collection</h1>

<p>Before starting the scraping process, it was essential to gather a comprehensive list of websites to be categorized. The data collection process involved the following steps:</p>

<h2 id="1-extracting-url-logs-from-internal-sources">1. Extracting URL Logs from Internal Sources</h2>

<p>As the firewall was already implemented in multiple colleges and companies, we were able to access URL logs from two colleges: <strong>GNDEC Ludhiana</strong> and <strong>Thapar University</strong>. These logs contained URLs visited by users over the last 3 years. The objective was to extract domain names from these logs for further processing.</p>

<h3 id="key-steps">Key Steps:</h3>
<ul>
  <li><strong>Log Extraction</strong>: We obtained the URL logs from the colleges’ firewall or network monitoring system. These logs contained data about every website visited by users.</li>
  <li><strong>Data Range</strong>: The data provided logs of URLs accessed over the last 3 years, which gave us a historical snapshot of website traffic.</li>
</ul>

<h2 id="2-extracting-domain-names">2. Extracting Domain Names</h2>

<p>After extracting the logs, the next task was to extract only the domain names from the URLs. This was crucial, as the logs also contained full URLs, but we only needed the domain portion for further processing.</p>

<h3 id="key-steps-1">Key Steps:</h3>
<ul>
  <li><strong>Parsing URLs</strong>: Using Python’s <code class="language-plaintext highlighter-rouge">urlparse</code> module or similar tools, we parsed the URLs to extract the domain names.</li>
  <li><strong>Handling Subdomains</strong>: We removed subdomains from the extracted domain names to ensure we were only left with the primary domain. For example, “sub.example.com” would be reduced to “example.com”.</li>
</ul>

<h2 id="3-filtering-and-cleaning-the-data">3. Filtering and Cleaning the Data</h2>

<p>Once we had the domain names, we performed a series of cleaning steps to ensure that only relevant domains were included.</p>

<h3 id="key-steps-2">Key Steps:</h3>
<ul>
  <li><strong>Removing Useless Domains</strong>: We filtered out domains that were irrelevant to the categorization task, such as domains for hosting services (e.g., “example-hosting.com”) and CDN websites (e.g., “cdn.example.com”).</li>
  <li><strong>Duplicate Removal</strong>: We removed any duplicate domains to avoid having redundant entries in our final list.</li>
  <li><strong>Final Cleaned Dataset</strong>: After cleaning, we were left with a list of several hundred thousand unique domain names.</li>
</ul>

<h2 id="4-dealing-with-expired-domains">4. Dealing with Expired Domains</h2>

<p>One of the challenges we encountered was that many of the domain names in the extracted list were no longer reachable, as they had expired. These expired domains could not be scraped, so we had to take steps to gather active domains for our project.</p>

<h3 id="key-steps-3">Key Steps:</h3>
<ul>
  <li><strong>Domain Availability Check</strong>: We implemented a script to check the availability of domains in the list. This helped identify and remove expired or unreachable domains.</li>
  <li><strong>Filtering Unreachable Domains</strong>: Domains that were no longer available or had expired were removed from the list, leaving only the active ones.</li>
</ul>

<h2 id="5-downloading-the-top-10-million-domains">5. Downloading the Top 10 Million Domains</h2>

<p>To complement the internal data from the URL logs, we downloaded a list of the top 10 million domains to increase the breadth of our dataset. This list was sourced from <a href="https://www.domcop.com/top-10-million-websites">Domcop’s Top 10 Million Websites</a>.</p>

<h3 id="key-steps-4">Key Steps:</h3>
<ul>
  <li><strong>Downloading from Domcop</strong>: We obtained a ready-made list of the top 10 million domains. This list contained domains that are among the most popular and widely visited on the web, making it a valuable source for website categorization.</li>
</ul>

<h2 id="6-challenges-with-alternative-data-sources">6. Challenges with Alternative Data Sources</h2>

<p>While exploring additional sources, we also attempted to download domain lists from Kaggle and GitHub. However, these sources presented some challenges.</p>

<h3 id="key-issues">Key Issues:</h3>
<ul>
  <li><strong>Missing Protocol</strong>: The lists from Kaggle and GitHub only contained domain names (e.g., “example.com”) without the “http” or “https” prefix. For web scraping, it is essential to have the full URLs, including the protocol (i.e., “https://example.com”). Without this information, the domains were not directly usable for scraping.</li>
  <li><strong>Data Inconsistencies</strong>: Some of the domain lists from Kaggle and GitHub lacked necessary details or had inconsistent formatting, which made them less practical compared to the Domcop list.</li>
</ul>

<h2 id="7-final-domain-list">7. Final Domain List</h2>

<p>After performing the data collection and cleaning steps, we were left with a refined list of domains, including the top 10 million domains from Domcop and the active domains from the internal URL logs. This list served as the foundation for the next stages of our website categorization project.</p>

<h3 id="key-steps-5">Key Steps:</h3>
<ul>
  <li><strong>Combining Sources</strong>: We combined the internal domain list with the top 10 million domains from Domcop to create a comprehensive list.</li>
  <li><strong>Pre-processing</strong>: We ensured that each domain in the final list had the proper format (including “http” or “https”) to ensure smooth scraping and data collection.</li>
</ul>
