---
layout: default
title: "Setup of RTX 4090 and Model Experimentation"
---

## Introduction

Yesterday, we successfully installed the **RTX 4090 GPU**. In order to support the GPU, we also had to purchase a **new power supply**. For our new setup, we are running **Ubuntu 24.04 LTS**, which comes with **preinstalled CUDA** and **NVIDIA graphics drivers**, streamlining the installation process. The system is configured with **32 GB of RAM** and a **2 TB SSD**, providing ample resources for our model development work.

## Model Download and Initial Testing

Today, we began testing the new setup by downloading the **llama3:8B-instruct-16fp** model, which is **16 GB in size**. With the RTX 4090's power, the **inference speed** of this model is extremely fast, measured in **milliseconds**. After experimenting with various prompts throughout the day, we found a **prompt** that works particularly well, allowing the model to consistently achieve an accuracy of over **94%**.

## Experiment: Combining Meta Zero Shot Model and Llama3

Toward the end of the day, we conducted an experiment where we attempted to **combine the Meta Zero Shot model** with the **Meta Llama model** for website categorization.

### Approach
1. **Meta Zero Shot**: The initial step was to use the Meta Zero Shot model to predict the category of a given website.
2. **Meta Llama**: After categorization, we would send both the **website content** and the **predicted category** to **llama3**. The goal was to ask the model whether the predicted category was correct, and if it wasn’t, to provide the correct category.

### Results
Unfortunately, this approach resulted in a **decrease in the accuracy** of the llama3 model. It seems that the **given predicted category** may have influenced the performance of the llama model, causing it to produce less accurate results. 

As a result, we decided to **drop this combined approach** and use **llama3 in isolation** for website categorization, where it continues to perform with **high accuracy**.

## Conclusion

- The new RTX 4090 setup has significantly improved our workflow, particularly with the **llama3:8B-instruct-16fp model**, achieving fast inference and **94%+ accuracy**.
- The combination of the **Meta Zero Shot** and **Llama3** models didn’t yield the desired results, so we will proceed with using **llama3 in isolation** moving forward.

