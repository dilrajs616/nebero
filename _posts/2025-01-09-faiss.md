---

layout: default  
title: "K-Means with FAISS (Facebook AI Similarity Search): Procedure and Results"

---

## Overview

After experimenting with traditional **K-Means** and **DBSCAN** clustering algorithms, we turned to **FAISS** (Facebook AI Similarity Search) to improve the clustering performance. FAISS is a library developed by Facebook for efficient similarity search and clustering of large datasets, especially those that are high-dimensional. We implemented **K-Means in FAISS** to see if we could achieve better performance and quality of clusters. The results showed a **slightly higher Silhouette Score than K-Means**, but the overall **quality of clusters** was still not satisfactory.

---

## What is FAISS?

**FAISS (Facebook AI Similarity Search)** is an open-source library designed for fast, efficient similarity search and clustering, particularly suited for handling very large datasets and high-dimensional vectors. FAISS is widely used in applications where **approximate nearest neighbors (ANN)** and vector similarity search are needed, such as recommendation systems, image search, and large-scale text search.

FAISS supports both:
- **Exact K-Means**: Standard K-Means clustering that computes exact distances between points and centroids.
- **Approximate K-Means**: An approximation of K-Means using fast nearest neighbor search techniques, which helps in clustering massive datasets quickly with a tradeoff between speed and accuracy.

### Key Features of FAISS:
1. **Scalability**: FAISS is highly optimized for handling very large datasets, making it an ideal choice for clustering tasks where traditional algorithms struggle with memory or computational limits.
2. **GPU Support**: FAISS can be accelerated with GPUs, enabling faster computation of clustering on very large datasets.
3. **Approximate Nearest Neighbor Search**: FAISS uses efficient algorithms like Inverted File Index (IVF) and Product Quantization (PQ) to perform approximate nearest neighbor searches.

---

## K-Means in FAISS: How It Works

When we applied **K-Means** using FAISS, we followed a similar approach to traditional K-Means but with significant optimizations for speed and scalability. Here’s how FAISS K-Means works and how we applied it to our dataset:

### Step-by-Step Procedure

1. **Data Preparation**:
   - We first transformed our dataset into numerical vectors using **TF-IDF** for text features. Each document was represented as a high-dimensional vector, which was used as input to the FAISS K-Means algorithm.
   - Preprocessing steps such as **stop-word removal** and **normalization** were performed as with traditional K-Means.

2. **FAISS K-Means Initialization**:
   - We initialized the **K-Means** algorithm in FAISS using the high-dimensional TF-IDF vectors as input.
   - Unlike traditional K-Means, FAISS allows us to choose between **exact K-Means** and **approximate K-Means**. We opted for **exact K-Means** for this experiment to ensure accuracy.

3. **Indexing and Search Optimization**:
   - FAISS uses optimized data structures such as the **Inverted File Index (IVF)** for efficient nearest neighbor search. This helps speed up the assignment of data points to clusters.
   - In cases where the dataset is very large, FAISS also supports **GPU acceleration** to further speed up the clustering process, but we didn’t require this for our dataset.

4. **Clustering Execution**:
   - FAISS performed K-Means clustering by iterating through the standard steps of:
     - **Assigning each data point** to the nearest cluster centroid.
     - **Updating the centroids** based on the mean of the points in each cluster.
   - This process was repeated until the centroids converged, and no significant changes occurred in the cluster assignments.

5. **Evaluation with Silhouette Score**:
   - After clustering, we calculated the **Silhouette Score** to evaluate how well FAISS K-Means performed in separating the clusters.
   - The **Silhouette Score** with FAISS K-Means was slightly higher than traditional K-Means, but it was still not ideal. The improvement was marginal, and the clusters were still not well-separated, as reflected in the final score.

---

## Silhouette Score: FAISS vs Traditional K-Means

As with traditional K-Means, we used the **Silhouette Score** to evaluate the performance of FAISS K-Means. The score provides an indication of how well-defined the clusters are by measuring **cohesion** (how close points are within a cluster) and **separation** (how far apart different clusters are).

- **Traditional K-Means**: Silhouette Score was **0.2**, indicating poor clustering.
- **FAISS K-Means**: Silhouette Score was slightly higher than **0.2**, but the improvement was marginal.

### Why Was the Silhouette Score Still Low?

Despite using FAISS for clustering, the **Silhouette Score** remained low. Here are some potential reasons:
- **Data Distribution**: The underlying data distribution may not be naturally suited to spherical clusters, which K-Means assumes. Our data likely contains complex patterns or uneven densities that K-Means struggles to capture.
- **High Dimensionality**: The high-dimensional nature of the TF-IDF vectors can cause problems for K-Means because, in high-dimensional spaces, distances between points tend to become more uniform, making it harder for the algorithm to separate clusters effectively.
- **Feature Selection**: The features used in the dataset (based on TF-IDF) might not be capturing the most relevant aspects of the data. It may be necessary to further refine feature selection or use dimensionality reduction techniques like **PCA** before clustering.

---

## Why Use FAISS K-Means?

### Pros:
- **Scalability**: FAISS excels at clustering large datasets, thanks to its optimized nearest neighbor search algorithms and GPU support. It’s particularly useful when traditional K-Means struggles with performance on large datasets.
- **Speed**: FAISS provides significant speed improvements, especially when using approximate nearest neighbor search or GPU acceleration.
- **Exact and Approximate Search**: You can choose between exact K-Means (for accuracy) and approximate K-Means (for speed), making FAISS flexible for different use cases.

### Cons:
- **Marginal Improvement in Clustering Quality**: While FAISS improves speed and scalability, it may not significantly improve clustering quality in certain datasets. In our case, the **Silhouette Score** was only slightly better than traditional K-Means, indicating that the core clustering problem remained unresolved.
- **Complexity in Tuning**: FAISS provides many parameters for fine-tuning, such as the type of indexing and search method, which can add complexity to the implementation.

---

## Conclusion

We applied **K-Means using FAISS** in an attempt to improve the clustering quality over traditional K-Means. While FAISS provided a **marginal improvement** in terms of the **Silhouette Score**, the overall **quality of clusters** was still not good enough for our purposes. The **Silhouette Score** remained low, indicating that the clusters were not well-defined.

FAISS’s ability to scale efficiently and handle large datasets makes it an attractive option, but it did not solve the fundamental problem of poor cluster separation in our case. Further refinement of feature selection, dimensionality reduction, or switching to a more sophisticated clustering algorithm may be necessary to achieve better results.

