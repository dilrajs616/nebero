---
layout: default
title: "Requirement Analysis"
---

### Overview

Initially, we evaluated the hardware requirements for running **LLaMA:3.1-405B-fp16** and **Deepseek-V3-671B-fp16** models. The hardware specifications were extremely high and unfeasible for our setup:

- **LLaMA:3.1-405B-fp16**: Required significant computational power.
- **Deepseek-V3-671B-fp16**: Demanded even higher resources.

Given these impractical requirements, we opted for a more manageable model—**LLaMA:3-8B-fp16**.

### New Model Decision: LLaMA:3-8B-fp16

After re-evaluating the project’s needs and considering available resources, we decided to use **LLaMA:3-8B-fp16**, which requires **16 GB of graphics memory**. Additionally, the **Facebook Zero Shot model** requires **2 GB of VRAM**, bringing the total requirement to **18 GB of VRAM**.

### GPU Decision: NVIDIA RTX 4090

To meet these new requirements, we chose the **NVIDIA RTX 4090**, which provides **24 GB of VRAM**. This model not only covers our current needs but also leaves additional memory capacity for future scalability.

#### Why RTX 4090?
- **24 GB VRAM**:  
   Easily handles both the **LLaMA:3-8B-fp16** (16 GB) and **Facebook Zero Shot model** (2 GB) with room to spare for future workloads.

   Example: With 24 GB of VRAM, we can run both models simultaneously, processing large datasets without memory limitations.

- **Future-Proofing**:  
   The extra VRAM allows for potential future upgrades or additional AI models as the project expands.

### Next Steps
- **Install and Configure RTX 4090**.
- **Test the models** to ensure they perform optimally with the new hardware.
- **Monitor VRAM usage** to evaluate potential scaling needs in future projects.

